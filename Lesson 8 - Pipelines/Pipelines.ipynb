{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have explored a series of steps that is useful for your task—or for a related family of tasks—you would like to package those in a less ad hoc and more reusable way.  Certainly wrapping a set of steps in a simple factory function is not difficult.  But for most flexibility it is best to take advantage of scikit-learn **pipelines**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imperative Sequential Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us repeat a similar construction of building and training a model as we have seen previously.  Here we carry out the steps imperatively, in the sequence we worked out in previous lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Some libraries tend to be in flux for their dependency versions\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "First load the data; this step will not become part of the pipeline since loading the initial data may occur in various ways that are use dependent.  We will use a cross validation score rather than do a train/test split up front.  In the next lesson we will see how this can be more robust than a simple train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# From here on, we refer to features and target by the\n",
    "# generic X, y rather than tie it to the dataset\n",
    "X, y = cancer.data, cancer.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Features\n",
    "\n",
    "We think the model may perform better with polynomial features that get at the interactions of multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 496)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Data\n",
    "\n",
    "Scale the data for better performance in subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 496)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# compute minimum and maximum on the training data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_poly)\n",
    "# rescale training data\n",
    "X_poly_scaled = scaler.transform(X_poly)\n",
    "X_poly_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Most Important Engineered Features\n",
    "\n",
    "Since we have increased the number of features to an unweildy number, let us select only the top most important few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 99)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "select = SelectPercentile(percentile=20)\n",
    "select.fit(X_poly_scaled, y)\n",
    "X_selected = select.transform(X_poly_scaled)\n",
    "X_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Feature Engineered Data Against Model\n",
    "\n",
    "Having gone through those steps, we would like to see how our engineered dataset performs on a model that showed some success in earlier lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(max_depth=7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CV scores: [0.92783505 0.99236641 0.98648649 0.98809524 0.97076023]\n",
      "Mean score: 0.9731086844519972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(rfc, X_selected, y, scoring=scorer, cv=kf)\n",
    "print(\" CV scores:\", cv_scores)\n",
    "print(\"Mean score:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let us see how we would have performed on the raw data.  This gets a moderate but significant improvement over the raw data in F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data CV scores: [0.88235294 0.95384615 0.97959184 0.98203593 0.97674419]\n",
      "    Raw mean score: 0.9549142091895085\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_val_score(rfc, X, y, scoring=scorer, cv=kf)\n",
    "print(\"Raw data CV scores:\", cv_scores)\n",
    "print(\"    Raw mean score:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pipelines\n",
    "\n",
    "A pipeline is simply an abstraction in scikit-learn to bundle together steps like those used above into a single model interface, following the same APIs as a model itself.  A particular pipeline is likely to be somewhat domain specific in that you may learn that those particular steps are useful for e.g. cancer data, but not as useful for data with very different characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/pipeline-diagram.png\" alt=\"Pipeline Illustration\" width=\"75%\"/>\n",
    "\n",
    "Image credit (CC-BY-NA): [Karl Rosaen](http://karlrosaen.com/ml/learning-log/2016-06-20/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "We can easily construct a pipeline consisting of just those steps (instances that follow the scorer interface) we want.  When we instantiate the various classes, we are free to pass in parameters we know we will want; these likely reflect our previous exploration of the particular domain and its datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"Polynomial features\", PolynomialFeatures(2)),\n",
    "    (\"MinMax scaling\", MinMaxScaler()),\n",
    "    (\"Top 20% features\", SelectPercentile(percentile=20)),\n",
    "    (\"Random Forest\", RandomForestClassifier(max_depth=7)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pipeline object is just like using a plain model, but all the preparation steps are bundled into a single object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pipeline CV scores: [0.92929293 0.96923077 0.97986577 0.98224852 0.96428571]\n",
      "Pipeline mean score: 0.9649847410663105\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_val_score(pipe, \n",
    "                            X, y, \n",
    "                            scoring=make_scorer(f1_score), \n",
    "                            cv=KFold(5))\n",
    "\n",
    "print(\" Pipeline CV scores:\", cv_scores)\n",
    "print(\"Pipeline mean score:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recover (and even modify in-place) the steps of a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Polynomial features',\n",
       "  PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)),\n",
       " ('MinMax scaling', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       " ('Top 20% features', SelectPercentile(percentile=20,\n",
       "           score_func=<function f_classif at 0x000001FB5332DD08>)),\n",
       " ('Random Forest',\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can serialize the pipeline for later use\n",
    "from pickle import dump, load\n",
    "dump(pipe, open('data/cancer-pipeline.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pipeline CV scores: [0.92929293 0.98484848 0.98666667 0.96428571 0.9704142 ]\n",
      "Pipeline mean score: 0.9671015992554454\n"
     ]
    }
   ],
   "source": [
    "newpipe = load(open('data/cancer-pipeline.pkl','rb'))\n",
    "cv_scores = cross_val_score(newpipe, \n",
    "                            X, y, \n",
    "                            scoring=make_scorer(f1_score), \n",
    "                            cv=KFold(5))\n",
    "\n",
    "print(\" Pipeline CV scores:\", cv_scores)\n",
    "print(\"Pipeline mean score:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('Polynomial features', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('MinMax scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('Top 20% features', SelectPercentile(percentile=20,\n",
       "         score_func=<function f_classif at 0x000001FB5332DD08>)), ('Random ...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A pipeline factory\n",
    "\n",
    "There is a convenience function to create pipelines.  The only difference of interest with the class constructor is that names of steps are generated for you rather than being explicitly spelled when you create a pipeline.  This is slightly more convenient but also takes away your option of giving more descriptive names for steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('polynomialfeatures',\n",
       "  PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)),\n",
       " ('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       " ('selectpercentile', SelectPercentile(percentile=20,\n",
       "           score_func=<function f_classif at 0x000001FB5332DD08>)),\n",
       " ('randomforestclassifier',\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(\n",
    "    PolynomialFeatures(2),\n",
    "    MinMaxScaler(),\n",
    "    SelectPercentile(percentile=20),\n",
    "    RandomForestClassifier(max_depth=7))\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines with Grid Search\n",
    "\n",
    "A very nice feature of using a pipeline is that it \"plays well\" with a grid search.  In fact, you are not restrained to searching over the hyperparameters of the model step, but can also search over arguments to other steps in the pipeline.  For this, spelling is a little easier if we use the generated step names from `make_pipeline()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross-validation accuracy: 0.9753954305799648\n",
      "best dataset score:  0.9982425307557118\n",
      "best parameters:  {'polynomialfeatures__degree': 3, 'randomforestclassifier__criterion': 'entropy', 'randomforestclassifier__max_depth': 9, 'selectpercentile__percentile': 15}\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Takes about a minute for this grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "          'selectpercentile__percentile': [10, 15, 20, 50],\n",
    "          'randomforestclassifier__max_depth': [5, 7, 9],\n",
    "          'randomforestclassifier__criterion': ['entropy', 'gini']}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=params, cv=5)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"best cross-validation accuracy:\", grid.best_score_)\n",
    "print(\"best dataset score: \", grid.score(X, y))   # Overfitting against entire dataset\n",
    "print(\"best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Grid CV scores: [0.94845361 0.98461538 0.97333333 0.98823529 0.97674419]\n",
      "Grid mean score: 0.9742763612720597\n"
     ]
    }
   ],
   "source": [
    "model = grid.best_estimator_\n",
    "cv_scores = cross_val_score(model, \n",
    "                            X, y, \n",
    "                            scoring=make_scorer(f1_score), \n",
    "                            cv=KFold(5))\n",
    "\n",
    "print(\" Grid CV scores:\", cv_scores)\n",
    "print(\"Grid mean score:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we select as `.best_estimator_` is itself a pipeline; it simply has been re-parameterized from the original pipeline, using the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('polynomialfeatures',\n",
       "  PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)),\n",
       " ('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       " ('selectpercentile', SelectPercentile(percentile=15,\n",
       "           score_func=<function f_classif at 0x000001FB5332DD08>)),\n",
       " ('randomforestclassifier',\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "              max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the relative success of all the parameter combinations as well.  As we saw in a prior lesson, `.cv_results_` contains a rich collection of information beyond this also.  Although the highest degree polynomial features and highest percentage feature selection was the best estimator, in the ranking of classifiers, there is quite a bit of variation in all the parameters searched.  In particular, entirely different combinations perform only slightly worse in the example (they are close enough that it might turn out differently among the top few with different random seeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>degree</th>\n",
       "      <th>criterion</th>\n",
       "      <th>depth</th>\n",
       "      <th>percentile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                degree criterion depth percentile\n",
       "rank_test_score                                  \n",
       "1                    3      gini     5         10\n",
       "1                    3   entropy     9         15\n",
       "3                    3   entropy     5         10\n",
       "3                    3   entropy     5         20\n",
       "5                    3   entropy     7         20\n",
       "5                    2      gini     5         10\n",
       "5                    2      gini     7         50\n",
       "5                    3      gini     5         20\n",
       "5                    3   entropy     9         50\n",
       "5                    3   entropy     5         15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid = pd.DataFrame(grid.cv_results_).set_index('rank_test_score').sort_index()\n",
    "df_params = df_grid.loc[:,df_grid.columns.str.contains('param_')]\n",
    "cols = [c.split('_')[-1] for c in df_params.columns]\n",
    "df_params.columns = cols\n",
    "df_params.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next lesson\n",
    "\n",
    "**Robust train/test splits**: In this lesson we looked at the very useful pipeline interface provided by scikit-learn.  Using pipelines greatly aids in making models and processing steps reproducible and easy to distribute to colleagues (or to yourself with later projects).\n",
    "\n",
    "The next and final lesson of this course, on robust train/test splits, will look at a variety of capabilities in scikit-learn for performing divisions between training and validation data that go beyond the basic function we used in most of these lessons.\n",
    "\n",
    "<a href=\"TrainTest.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
